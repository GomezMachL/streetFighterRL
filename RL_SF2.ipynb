{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b87b5de7",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Street Fighter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dde5cee",
   "metadata": {},
   "source": [
    "## Configurar entorno Street Fighter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc63aa",
   "metadata": {},
   "source": [
    "### Instalar y configurar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90948d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos los warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Forzar uso CPU en caso de dispones GPU en el PC\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3690f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instalar librerias\n",
    "!pip3 install gym gym-retro\n",
    "#Instalar con pip3, con pip dio problemas al reconocer las roms\n",
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "!pip3 install optuna\n",
    "!pip3 install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3ae23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import retro para crear el entorno Street Fighter a partir de la ROM\n",
    "import retro\n",
    "\n",
    "# Import Time para relentizar el juego\n",
    "import time\n",
    "\n",
    "# Comandos a ejecutar en el terminal\n",
    "# Navegar a la carpeta que contiene/contendrá las roms y descomprimir los .zip con las roms\n",
    "# Path de la carpeta = ./videoGames_roms/roms\n",
    "# Ejecutar python -m retro.import . (debe reconocer las roms .md, en caso de que sea para la Sega Genesis,\n",
    "# si no reconoce se puede ejecutar pip3 install gym gym-retro en la terminal del entorno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b3a86",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# verificamos las roms que están importadas\n",
    "retro.data.list_games()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32044eee",
   "metadata": {},
   "source": [
    "### Comprobar el entorno SF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cerramos la variable env por si existiera una instancia\n",
    "# env.close()\n",
    "\n",
    "# Instanciamos el entorno, no se pueden intanciar varios entornos por ejecución\n",
    "env = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis')\n",
    "\n",
    "# Para instanciar varios entornos en paralelo utilizar retrowrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102daef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "for game in range(1):\n",
    "    while not done:\n",
    "        if(done):\n",
    "            obs = env.reset()\n",
    "            \n",
    "        env.render()\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "        time.sleep(0.01)\n",
    "        print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece143b",
   "metadata": {},
   "source": [
    "#### Métodos útiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ea8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.observation_space() # Verificar entorno\n",
    "# env.action_space() # Acciones disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e7eec2",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b5fd9f",
   "metadata": {},
   "source": [
    "### Preprocesado del entorno\n",
    "Con el fin de agilizar el entrenamiento de agente vamos a preprocesar el entorno: comprimiendo los datos del entorno (reduciendo el frame), calculando la variación de los pixel del frame actual con respecto al último para capturar movimiento y aplicarle una escala de grises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerias\n",
    "# Import Clase base del entorno para hacer wrapper\n",
    "from gym import Env\n",
    "\n",
    "# Import los shapes espaciales para el entorno\n",
    "from gym.spaces import MultiBinary, Box\n",
    "\n",
    "# Import numpy para calcular el frame delta\n",
    "import numpy as np\n",
    "\n",
    "# Import opencv para aplicar la escala de grises\n",
    "import cv2\n",
    "\n",
    "# Import matplotlib para visualizar la imagen\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d710ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una clase para definir el entorno de SF2\n",
    "class StreetFighter(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Especificar el espacio de acciones y el espacio de observaciones\n",
    "        self.observation_space = Box(low=0, \n",
    "                                     high=255, \n",
    "                                     shape=(84, 84, 1), \n",
    "                                     dtype=np.uint8)\n",
    "        \n",
    "        self.action_space = MultiBinary(12)\n",
    "        \n",
    "        # Instanciar el entorno\n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis',\n",
    "                               use_restricted_actions=retro.Actions.FILTERED)\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        # Devolvemos el primer frame\n",
    "        obs = self.game.reset()\n",
    "         \n",
    "        # Preprocess\n",
    "        obs = self.preprocess(obs)\n",
    "        self.previous_frame = obs\n",
    "        \n",
    "        # Inicializar atributo para la diferencia de la puntuación\n",
    "        self.score = 0\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def preprocess(self, observation):\n",
    "        # Aplicamos el redimensionado del frame y el escalado de grises\n",
    "        # Escalado de grises\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        # Redimensionado del frame\n",
    "        resize = cv2.resize(gray, (84,84), interpolation=cv2.INTER_CUBIC)\n",
    "        # Añadir el valor de los canales\n",
    "        channel = np.reshape(resize, (84,84,1))\n",
    "        \n",
    "        return channel\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Realizar una acción\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        \n",
    "        # Procesamos la observación\n",
    "        obs = self.preprocess(obs)\n",
    "        \n",
    "        # Calcular frame delta (Variación en frame anterior y actual)\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs\n",
    "        \n",
    "        # Adaptamos la función de recompensa\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "        \n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95b1be-5605-437d-9e4b-374b45d2c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Clase SF2\n",
    "#env.close()\n",
    "env = StreetFighter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87173a-d087-4a7b-93c9-bca241399f94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "for game in range(1):\n",
    "    while not done:\n",
    "        if(done):\n",
    "            obs = env.reset()\n",
    "            \n",
    "        env.render()\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "        time.sleep(0.01)\n",
    "        if(reward > 0):\n",
    "            print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ac07d-7667-42f6-a832-04042a933ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f7b0e0-74b8-4e84-810f-b2a4db44c7da",
   "metadata": {},
   "source": [
    "### Optimización de Hiperparámetros\n",
    "En este apartado optimizaremos los siguientes hiperparametros del modelo utilizando Optuna:\n",
    "+ n_step: número máximo de acciones del episodio.\n",
    "+ gamma: contiene el parémetro que reduce la recompensa por cada acción realizada. \n",
    "+ learning_rate: ratio de aprendizaje del algoritmo. \n",
    "+ clip_range: \n",
    "+ gae_lambda: parámetro de suavización de la ventaja "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d5e0f-8d99-46c6-8498-dc8b6952113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerias\n",
    "\n",
    "# Importar optuna: frame para optimización de parrámetros\n",
    "import optuna\n",
    "\n",
    "# Importar algoritmo de entranamiento\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Importar métrica de evaluación del modelo\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Importar libreria de Baselines para monitorización\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Import the vec wrappers to vectorize and frame stack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2137ddb1-24fd-49ff-8792-e81226474b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './log/'\n",
    "OPT_DIR = './opt/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfff803a-bb2f-4592-a017-acd4f0766982",
   "metadata": {},
   "source": [
    "#### Función para determinar los hiperparámetros a optimizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e52649-0b5a-4695-a82b-b82e1b2ea896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_ppo(trial):\n",
    "    return {\n",
    "        'n_steps': trial.suggest_int('n_steps',2048,8192),\n",
    "        'gamma': trial.suggest_loguniform('gamma',0.8,0.9999),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate',1e-5,1e-4),\n",
    "        'clip_range': trial.suggest_uniform('clip_range',0.1,0.4),\n",
    "        'gae_lambda': trial.suggest_uniform('gae_lambda',0.8,0.99)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a174348-1f61-47ed-818c-1bff37319070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent(trial):\n",
    "    try:\n",
    "        \n",
    "        model_params = optimize_ppo(trial)\n",
    "        \n",
    "        # Inicializar entorno\n",
    "        env = StreetFighter()\n",
    "        env = Monitor(env,LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "        \n",
    "        # Definir algoritmo\n",
    "        model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        #model.learn(total_timesteps=30000)\n",
    "        model.learn(total_timesteps=100000)\n",
    "        \n",
    "        # Evaluación algoritmo\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "        env.close()\n",
    "        \n",
    "        SAVE_PATH = os.path.join(OPT_DIR,'trial_{}_bestModel'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "        return mean_reward\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae35a1e2-dfe5-48d6-a974-1d07e2be9aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creación del experimento\n",
    "study = optuna.create_study(direction='maximize')\n",
    "STUDY_PATH = \"./studies/\"\n",
    "#study = optuna.create_study(direction='maximize',storage=STUDY_PATH)\n",
    "#study.optimize(optimize_agent, n_trials= 10, n_jobs= 1)\n",
    "study.optimize(optimize_agent, n_trials= 100, n_jobs= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be835b-569a-4259-b79c-1f8f9d4e9d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Método para obtener los trails del estudio\n",
    "study.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e77bee2-1a3b-4680-964f-4fe9b5ca1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(os.path.join(OPT_DIR, 'trial_0_bestModel.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10920e9-e4a0-4639-864e-0762c8557633",
   "metadata": {},
   "source": [
    "### Configurar entrenamiento y donde almacenarlo \n",
    "En este apartado se definirán los bloques de entrenamiento y donde se almacenará el agente resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9f751-ae56-48c6-bf41-8209e37fb3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar base callbacks\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0960de1-b45d-4705-898d-2e9eb0ef0dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c897381-480f-4d98-8ab9-55d2a1d14659",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1116d8-d318-401a-bd86-ea1223a5718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b895d02-c3e5-4827-a8d7-60aa5a68488e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Entrenar el agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd61c1b7-c0b5-4449-9d84-b5e2c7e4e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment \n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "model_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f971f-bb2d-4372-8a53-3aef2dcaf1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b903a-cb5e-469b-bd3d-ca8548d9514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "92*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f242b9b8-6687-43b8-8b5e-ab7ee5961817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos un número de mini-batch que sea multiplo del tamaño del mini-batch para evitar truncar los mini-batch \n",
    "# y reducimos el ratio de aprendizaje para evitar el sobreaprendizaje\n",
    "model_params['n_steps'] = 5888\n",
    "model_params['learning_rate'] = 8.643050555325267e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bd1562-0563-4c86-91d5-cc9f912a4478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n",
    "\n",
    "# Cargamos el mejor modelo \n",
    "model.load(os.path.join(OPT_DIR, 'trial_0_bestModel.zip'))\n",
    "\n",
    "model.learn(total_timesteps=100000, callback=callback)\n",
    "#model.learn(total_timesteps=1000000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3238514-19b3-4171-ac90-48a3e3d28514",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluación del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6e50d-6abc-4735-8738-a305aa45c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('./train/best_model_10000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa7e2e-7b17-4073-ba6d-d541debcad1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_reward, _ = evaluate_policy(model, env, render=True, n_eval_episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da10cb7b-a7d7-4153-9c20-e33d8f1d6698",
   "metadata": {},
   "source": [
    "### Afinamiento del modelo\n",
    "En este apartado se va a afinar el agente empleando uno de los agentes resultantes del entrenamiento, para ello se reducirá el ratio de aprendizaje para evitar sobreentrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bcda55-e4d7-4e26-8006-c2b0e2e09462",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPROVE_PATH = './improveModel/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3136d8b-6b12-41e2-ba98-3acc6c16256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducimos el learning_rate de aprendizaje para disminuir la capacidad de aprendizaje del modelo y evitar sobre entrenamiento\n",
    "model_params['learning_rate'] = 8.643050555325267e-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b75f8b-bf5f-48c6-96e0-439c0bf06c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "improve_callback = TrainAndLoggingCallback(check_freq=1000, save_path=IMPROVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74014e78-2e28-4198-98ce-586852c81198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n",
    "\n",
    "# Cargamos el mejor modelo \n",
    "model.load(os.path.join(CHECKPOINT_DIR, 'best_model_20000.zip'))\n",
    "\n",
    "model.learn(total_timesteps=100000, callback=improve_callback)\n",
    "#model.learn(total_timesteps=1000000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b48820-d24f-451a-9fd6-ef122e8dcc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('./improveModel/best_model_10000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b0368-d8f2-4323-acb0-7ec9bd9e72a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_reward, _ = evaluate_policy(model, env, render=True, n_eval_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f391f-cbf1-4209-b829-0f609b8adfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acff20f-115c-4447-bf2e-a22339b38e19",
   "metadata": {},
   "source": [
    "### Test Modelo\n",
    "En este apartado se van a proceder a realizar las pruebas del agente, para ello se realizará una partida y se comprobará el rendimiento del agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce4d93-6616-49a2-97dd-e29367097374",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc973b0-8975-4833-b4bc-34c05feba0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb6e3ec-c33f-4d0f-a869-3e81110c0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set flag to flase\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a48ecd-b978-45a1-ab5e-a1e30f655636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reset game to starting state\n",
    "obs = env.reset()\n",
    "# Set flag to flase\n",
    "done = False\n",
    "for game in range(1): \n",
    "    while not done: \n",
    "        if done: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        #time.sleep(0.01)\n",
    "        if(reward > 0):\n",
    "            print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e3b14-d539-4c14-aaaa-0f9a5102688e",
   "metadata": {},
   "source": [
    "#### Resultados de las pruebas\n",
    "Después observar varios partidas del agente, se ha observado que el agente presenta problemas a la hora de calcular distancias, por ejemplo, cuando el oponente está muy cerca del personaje del agente y lanza un ataque de rango, el agente rápidamente lanza el otro para evitar ser golpeado. Por el contrario, cuando el oponente lanza un ataque de rango desde una distancia considerable, el agente responde saltando al momento que el contrincante lanza el ataque lo que provoca que al caer sea golpeado por este ataque, este problema se podría solucionar modificando la diferencia entre los frames (frame_delta) y de esta forma el frame delta podría contener la trayectoria del ataque y no su posición actual solo.\n",
    "\n",
    "Por otro lado, habría que analizar si el conjunto de acciones del modelo incluye desplazamiento lateral porque cuando este se desplaza lo hace saltando o realizando alguna acción durante el salto, realizar estos tipos de movimientos para desplazarte te deja vulnerable durante la caida.\n",
    "\n",
    "Por último, no se está puntuando de forma negativa cada vez que el agente recibe un golpe o realiza una acción y no obtiene un resultado positivo, podría ser buena idea puntuarle negativamente este tipo de acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b1636-deb3-4827-bd1c-d5bdd8edc458",
   "metadata": {},
   "source": [
    "### Notas Trabajo Fin de Máster\n",
    "Al igual que comenta Nicholas en el video para entrenar este tipo de modelos es necesario realizar un proceso de optimización de los hiperparámetros del orden de cien mil acciones y 100 procesos, y durante el entrnemiento realizar el proceso un millón de veces, a día de hoy no se ha podido hacer pero después de la entrega de Machine Learning avanzado, pretendo realizar este tipo de entrenamiento.\n",
    "\n",
    "Además creo que debería de guardarse el estudio de optuna para no perderlo y poder reanudar el proceso de ser necesario sin tener que rehacer el estudio de cero.\n",
    "\n",
    "Por otro lado, me he dado cuenta de que tenía un fallo en el calculo de la recompensa (estaba almacenando en la clase como score la recompensa y no el valor de la puntuación, \"self.score = reward\") por este motivo se obtenían como resultado puntuaciones tan altas.\n",
    "\n",
    "He observado que en tensorboard que a partir de los 100.000 aprendizajes el modelo la recompensa decae y no consigue mejorar, quiero realizar un entrenamiento como el que comenta Nicholas y analizar los distintos modelos que genera y como evolucionan en el proceso de entrnamiento.\n",
    "\n",
    "Me gustaría poder continuar con este proyecto como TFM, me ha parecido muy interesante y divertido verle mejorar, además me parece bastante interesante este tipo de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce93ed-bf0e-42cf-93ad-a4f19b0ad0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
